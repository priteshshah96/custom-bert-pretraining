# Custom BERT Pretraining

This project is focused on pretraining a BERT model with domain-specific knowledge. 

## Folder Structure
- `data/`: Contains raw, tokenized, and preprocessed data.
- `configs/`: Configuration files for the tokenizer and BERT model.
- `scripts/`: Python scripts for preprocessing, training, and evaluation.
- `models/`: Contains saved tokenizer and pretrained models.
- `logs/`: Logs generated during training.
- `checkpoints/`: Model checkpoints during training.

## Setup
1. Install dependencies using `requirements.txt` or `Poetry`.
2. Run the scripts in the `scripts/` directory for pretraining.

## Dependencies
Refer to `requirements.txt`.
