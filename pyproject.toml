[tool.poetry]
name = "custom-bert-pretraining"
version = "0.1.0"
description = "Pretraining BERT model on custom domain knowledge"
authors = ["Pritesh shah <priteshshahwork@gmail.com>"]
license = "MIT"
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
torch = "^2.5.1"
transformers = "^4.47.1"
datasets = "^3.2.0"
tokenizers = "^0.21.0"
tqdm = "^4.67.1"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
